\chapter{\abstractname}

%TODO: Abstract

As Multimodal Large Language Models (MLLMs) increasingly support 
UI code generation from visual inputs (e.g., UI screenshots), their impact 
in accelerating frontend development is growing. While prior work 
has explored the generation of functional and visually accurate code,
its accessibility remains less explored. This thesis 
investigated the accessibility of MLLM-generated HTML/CSS code from 
UI screenshots in an empirical study. It evaluates multiple 
state-of-the-art MLLMs with different prompting strategies 
across benchmark and real-world datasets. The study, presented 
in this thesis, investigates 
five research questions: (1) whether MLLMs can generate accessible 
code by default, (2) how model differences impact accessibility 
outcomes, (3) whether advanced prompting techniques improve 
accessibility, (4) how consistent accessibility 
violations are across different MLLMs given the same UI 
screenshot, and (5) the presence of potential data leakage
in model training. The findings show that even though MLLMs demonstrate 
high performance in code fidelity, they often fail to fulfill 
critical accessibility requirements. They also highlight common violations,
analyze prompting effects, and discuss implications for model training
and evaluation. Based on these findings, this thesis proposes future research
directions to enhance accessibility in AI-driven frontend development.
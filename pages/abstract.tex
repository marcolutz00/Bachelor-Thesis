\chapter{\abstractname}

%TODO: Abstract

Since Multimodal Large Language Models (MLLMs) increasingly support 
UI code generation from visual inputs, such as UI screenshots, their role 
in accelerating frontend development is growing. While prior work 
has explored the generation of functional and visually accurate code,
its accessibility remains less explored. This thesis 
investigated the accessibility of MLLM-generated HTML/CSS code from 
UI screenshots in an empirical study. We evaluate multiple 
state-of-the-art MLLMs with different prompting strategies 
across benchmark and real-world datasets. Our study investigates 
four research questions: (1) whether MLLMs can generate accessible 
code by default, (2) how model differences impact accessibility 
outcomes, (3) whether advanced prompting techniques improve 
accessibility, and (4) the presence of potential data leakage
in model training. Our findings show that even though MLLMs demonstrate 
high performance in code fidelity, they often fail to fulfill 
critical accessibility requirements. We highlight common violations,
analyze prompting effects and discuss implications for model training
and evaluation. Based on our findings, we propose future research 
directions to enhance accessibility in AI-driven frontend development.
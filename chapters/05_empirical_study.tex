\chapter{Experiment}\label{chapter:Experiment}

\section{Experiment Setup}
In this thesis, we test the capabilities of LLMs to generate accessible code in an 
Image-to-Code environment. Therefore, a pipeline is used which provides the LLMs an 
image with instructions to replicate this image in HTML/CSS. In the following, 
advanced prompting techniques are also provided to guide the LLMs to generate
accessible code. 
The output of the LLMs for each parameter tuple (Model, Prompting Technique) 
is then analyzed on visual and structural 
similarity, as well as on accessibility compliance, based on WCAG 2.2 standards.
In order to cope with the probabilistic nature of LLMs, each experiment with a 
predefined tuple of parameters will be tested within 3 experiment runs.\newline
After the generation of the LLMs' output for the Image-to-Code task, the HTML/CSS
is first analyzed on visual and structural similarity as described above and afterwards
analyzed on accessibility violations with the help of axe-core, lighthouse and pa11y.


\subsection{Model Selection}
The selection of the LLMs is a decisive factor for the interpretation of the results.
To see the diffences of performance, we decided to use different types of 
state-of-the-art models. They differentiate in size, architecture and use case, but 
they all have vision capabilities in common.
In order to provide a general picture, we identified three model groups of interest:
\begin{itemize}
  \item \textbf{Commercial:} Big, commercial models build the foundation of our tests.
    We use \textit{gpt-4o} and \textit{gemini flash 2.0} as representatives of this group.
    They have not been specifically trained for Image-to-Code tasks, but prior papers 
    already show promising results in this field.
  \item \textbf{Small, Open-Source:} Small, open-source models build the second group. 
    Theoretically, they should be more accessible for the public and could be hosted 
    on local machines. The representatives of this group are \textit{llama 3.2 vision}
    and \textit{qwen 7B vl}. While the models are signifant smaller, they still have been 
    trained on a lot of data.
  \item \textbf{Fine-tuned:} The last group are fine-tuned models. In this case, we 
    consider models which have been trained specifically on similar Image-to-Code 
    tasks. We use \textit{VLM WebSight finetuned} which is a fine-tuned version official
    the models \textit{SigLIP} and \textit{Mistral 7B v0.1}. It has been trained on the 
    \textit{Websight} dataset which contains more than 800,000 HTML/CSS data entries. 
\end{itemize}

\section{Prompting Techniques}
In order to understand the models' capabilities to generate accessible code, we test
the models with different prompting techniques. Prior research has shown that more 
advanced prompting techniques can help to improve the generation of robust and 
accurate code. Therefore, we combine commonly used techniques with 
advanced prompting techniques to guide the LLMs to create more accessible code.
The wording of the prompts is similar to the one of prior research~\parencite{suh2025accessiblecode, xiao2024interaction2code}.
The content is the same while we have enriched the prompts with more details.\newline
All prompts can be seen in the appendix.

\subsection{Naive}
The naive approach only instructs the LLMs to accurately fulfill Image-to-Code tasks
without mentioning accessibility in its prompt. This shows how state-of-the-art 
LLMs perform in generating accessible code naturally without instructing it to 
do so.

\subsection{Zero-Shot}
The zero-shot approach resembles the naive approach in terms of the Image-to-Code
instructions. However, here the LLMs are explicitly instructed to obey the WCAG 
2.2 standards. The prompt does not contain examples, however it emphasizes 
accessibility by reminding it about the WCAG standards including a link to 
official WCAG standards.

\subsection{Few-Shot}
The few-shot approach resembles the zero-shot prompt, however it is enriched with 
explicit examples to support the LLM to generate more accessible code. 
Since the number of possible examples is too large, we decided to provide examples 
for only 8 of the most common accessibility violations. 
For this approach, we have followed the structure of previous 
works~\parencite{suh2025accessiblecode}.
The structure of the examples contains the rule's name, a description, a
correct example and a wrong counter example. If possible, the examples 
were taken from the official W3C website~\parencite{wcag22}.

\subsection{Reasoning}
The reasoning prompt is used to let the LLMs reason about the task and possible 
accessibility problems within the generation. Similar to prior work, we use 
a prompt to generate reasoning steps with \"Letâ€™s think step by 
step.\" as part of the output~\parencite{chae2024thinkexecute}.



\section{Advanced Strategies}
To solve the limitations of current prompting techniques, we test three 
advanced improvement strategies. One is an iterative approach that uses the 
feedback from accessibility tools in an iterative manner to improve the code.
The second is a composite approach which combines 
pre-processing and post-processing techniques to improve the LLMs' output.
The last approach is a multi-agent approach that uses further 
state-of-the-art LLMs to detect and solve accessibility violations. \newline
The advanced strategies are compared to old approaches and the capabilities 
of AI models to resolve accessibility violations are analyzed.

\subsection{Iterative Feedback}
The iterative approach follows a similar approach to recent findings in the area
of generating more accessible code~\parencite{suh2025accessiblecode}. This 
prompting technique lets the LLMs generate Image-to-Code tasks by using the naive 
prompting technique in the base iteration. Afterwards, the generated output is 
analyzed and accessibility violations found are incorporated with a refinement 
message to the LLMs. The objective is to instruct the LLMs to create a more 
accessible code based on the violations found in the code. The iterative approach 
contains one naive prompt and three rounds of code refinement. If the LLMs 
generate code without any accessibility violations within one round, the 
iterations stop.

\subsection{Color-Aware Feedback}
This approach uses an advanced block detection algorithm in order to receive
the UI text components and its corresponding bounding boxes in an image. This 
algorithm is based on previous works~\parencite{si2024design2code} and 
determines the components without using \textit{OCR} (Optical Character 
Recognition). With the help of the bounding boxes, the font color 
and the background color of each UI component is determined. On this basis,
the relative luminance and the color contrast ratio is calculated. If a 
color contrast ratio fails the requirements, a new color is recommended 
that fullfills this guideline.\newline
After the generation of the frontend code by LLMs, similar to the 
iterative approach, the code is analyzed for accessibility violations. The 
violations found are combined with a refinement message to serve as input 
for a refinement round. In contrast to the iterative approach, we only use 
one round of improvements.

\subsection{Iterative Multi-Agent}
In a combination with different agents, this approach uses a 3-layer architecture
to detect, identify and fix violations in the code. In comparison to the 
approaches above, it is fully based on LLMs and does not use any pre- or 
post-processing techniques. \newline
The first layer, called \textit{Issue Detector}, finds the place in the code which 
violates a certain standard and outputs the violations in a pre-defined format.
Based on this output, the second layer \textit{Issue Identifier} classifies the issues. 
It identifies them based on the WCAG 2.2 standards by 
adding the guideline's name and the severity of the violation. In a last 
layer, the \textit{Issue Resolver}, the violations are patched. In order to be 
as precise as possible, without adding new violations, we resolve the violations 
in batch sizes of \textit{n=5}.\newline
This architecture allows a clean separation between the location, the type and 
the solution to accessibility violations. Its structure can be seen in image xxy.

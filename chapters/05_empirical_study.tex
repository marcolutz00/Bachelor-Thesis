\chapter{Experiment}\label{chapter:Experiment}

\section{Experiment Setup}
In this thesis, we test the capabilities of LLMs to generate accessible code in an 
Image-to-Code environment. Therefore, a pipeline is used which provides the LLMs an 
image with instructions to replicate this image in HTML/CSS. In the following, 
advanced prompting techniques are also provided to guide the LLMs to generate
accessible code. 
The output of the LLMs for each parameter tuple (Model, Prompting Technique) 
is then analyzed on visual and structural 
similarity, as well as on accessibility compliance, based on WCAG 2.2 standards.
In order to cope with the probabilistic nature of LLMs, each experiment with a 
predefined tuple of parameters will be tested within 3 experiment runs.\newline
After the generation of the LLMs' output for the Image-to-Code task, the HTML/CSS
is first analyzed on visual and structural similarity as described above and afterwards
analyzed on accessibility violations with the help of axe-core, lighthouse and pa11y.


\subsection{Model Selection}
The selection of the LLMs is a decisive factor for the interpretation of the results.
To see the diffences of performance, we decided to use different types of 
state-of-the-art models. They differentiate in size, architecture and use case, but 
they all have vision capabilities in common.
In order to provide a general picture, we identified three model groups of interest:
\begin{itemize}
  \item \textbf{Commercial:} Big, commercial models build the foundation of our tests.
    We use \textit{gpt-4o} and \textit{gemini flash 2.0} as representatives of this group.
    They have not been specifically trained for Image-to-Code tasks, but prior papers 
    already show promising results in this field.
  \item \textbf{Small, Open-Source:} Small, open-source models build the second group. 
    Theoretically, they should be more accessible for the public and could be hosted 
    on local machines. The representatives of this group are \textit{llama 3.2 vision}
    and \textit{qwen 7B vl}. While the models are signifant smaller, they still have been 
    trained on a lot of data.
  \item \textbf{Fine-tuned:} The last group are fine-tuned models. In this case, we 
    consider models which have been trained specifically on similar Image-to-Code 
    tasks. We use \textit{VLM WebSight finetuned} which is a fine-tuned version official
    the models \textit{SigLIP} and \textit{Mistral 7B v0.1}. It has been trained on the 
    \textit{Websight} dataset which contains more than 800,000 HTML/CSS data entries. 
\end{itemize}

\section{Prompting Techniques}
In order to understand the models' capabilities to generate accessible code, we test
the models with different prompting techniques. Prior research has shown that more 
advanced prompting techniques can help to improve the generation of robust and 
accurate code. Therefore, we combine commonly used techniques with 
advanced prompting techniques to guide the LLMs to create more accessible code.
The wording of the prompts is similar to the one of prior research~\parencite{suh2025accessiblecode, xiao2024interaction2code}.
The content is the same while we have enriched the prompts with more details.\newline
All prompts can be seen in the appendix.

\subsection{Naive}
The naive approach only instructs the LLMs to accurately fulfill Image-to-Code tasks
without mentioning accessibility in its prompt. This shows how state-of-the-art 
LLMs perform in generating accessible code naturally without instructing it to 
do so.

\subsection{Zero-Shot}
The zero-shot approach resembles the naive approach in terms of the Image-to-Code
instructions. However, here the LLMs are explicitly instructed to obey the WCAG 
2.2 standards. The prompt does not contain examples, however it emphasizes 
accessibility by reminding it about the WCAG standards including a link to 
official WCAG standards.

\subsection{Few-Shot}
The few-shot approach resembles the zero-shot prompt, however it is enriched with 
explicit examples to support the LLM to generate more accessible code. 
Since the number of possible examples is too large, we decided to provide examples 
for only 8 of the most common accessibility violations. 
For this approach, we have followed the structure of previous 
works~\parencite{suh2025accessiblecode}.
The structure of the examples contains the rule's name, a description, a
correct example and a wrong counter example. If possible, the examples 
were taken from the official W3C website~\parencite{wcag22}.

\subsection{Reasoning}
The reasoning prompt is used to let the LLMs reason about the task and possible 
accessibility problems within the generation. Similar to prior work, we use 
a prompt to generate reasoning steps with \"Letâ€™s think step by 
step.\" as part of the output~\parencite{chae2024thinkexecute}.

\subsection{Iterative}
The iterative approach follows a similar approach to recent findings in the area
of generating more accessible code~\parencite{suh2025accessiblecode}. This 
prompting technique lets the LLMs generate Image-to-Code tasks by using the naive 
prompting technique in the base iteration. Afterwards, the generated output is 
analyzed and accessibility violations found are incorporated with a refinement 
message to the LLMs. The objective is to instruct the LLMs to create a more 
accessible code based on the violations found in the code. The iterative approach 
contains one naive prompt and three rounds of code refinement. If the LLMs 
generate code without any accessibility violations within one round, the 
iterations stop.


\section{Advanced Accessibility Strategies}
To solve the limitations of current prompting techniques, we test two 
advanced improvement strategies. One is a composite approach which combines 
pre-processing and post-processing techniques to improve the LLMs' output.
The second approach is a multi-agent approach that uses further 
state-of-the-art LLMs to detect and solve accessibility violations. \newline
The advanced strategies are compared to old approaches and the capabilities 
of AI models to resolve accessibility violations are analyzed.

\subsection{Color-Aware Iterative Approach}
This approach uses an advanced block detection algorithm in order to receive
the UI text components and its corresponding bounding boxes in an image. This 
algorithm is based on previous works~\parencite{si2024design2code} and 
determines the components without using \textit{OCR} (Optical Character 
Recognition). With the help of the bounding boxes, the font color 
and the background color of each UI component is determined. On this basis,
the relative luminance and the color contrast ratio is calculated. If a 
color contrast ratio fails the requirements, a new color is recommended 
that fullfills this guideline.\newline
After the generation of the frontend code by LLMs, similar to the 
iterative approach, the code is analyzed for accessibility violations. The 
violations found are combined with a refinement message to serve as input 
for a refinement round. In contrast to the iterative approach, we only use 
one round of improvements.

\subsection{Multi-Agent Approach}
In a combination with different agents, this approach uses a 3-layer architecture
to detect, identify and fix violations in the code. \newline
The first layer finds the place in the code which violates a certain standard.
Only the marked regions are submitted to the next layer. The second layer 
identifies the issues. It classifies them based on the WCAG 2.2 standards by 
adding the name, its severity and an explanation of the violation. In a last 
layer, the violations are patched with another agent. It creates a solution 
and an explanation to the user.\newline
This architecture allows a clean separation between the location, the type and 
the solution to accessibility violations. Its layout can be seen in image xxy.

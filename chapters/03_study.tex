\chapter{Empirical Study}\label{chapter:EmpiricalStudy}
Based on five research questions, a comprehensive empirical study 
was conducted to gain a better understanding of the 
capabilities and constraints of MLLMs in generating accessible UI code.
% To better understand the capabilities and limitations of 
% MLLMs in generating accessible UI code, a comprehensive 
% study guided by four research questions was conducted. 
The following presents the study setup and the quantitative and
qualitative results.

\section{RQ1: Do MLLMs generate accessible code from UI screenshots?}
\subsection{Experiment Setup}
To evaluate the baseline capability of MLLMs in generating accessible
code from visual UI input, two widely-used MLLMs are 
selected for this study: \textit{GPT-4o} and \textit{Gemini 2.0 Flash}.
These models show strong performance in 
multimodal tasks, and their support for image inputs makes
them suitable candidates for this study.\newline
As described in Section~\ref{chapter:Methodology}, the models are 
prompted with our dataset of UI screenshots and a corresponding 
task description. The task description includes a naive (plain)
prompt, which instructs the model to generate HTML/CSS code
based on the provided screenshot, without mentioning accessibility.

\newmdenv[
  backgroundcolor=lightgray,
  linecolor=black,
  linewidth=0.5pt,
%   roundcorner=20pt,
  innertopmargin=3pt,
  innerbottommargin=3pt,
  innerleftmargin=10pt,
  innerrightmargin=10pt,
  skipbelow=3pt,
  skipabove=3pt,
]{roundedbox}

\begin{roundedbox}
\textbf{Naive Prompt}: Your task is to replicate 
the provided UI screenshot of a webpage pixel-perfectly using 
HTML/CSS.
\end{roundedbox}

This setup allows to observe whether MLLMs can 
naturally produce accessible code without direct instruction 
and acts as a benchmark for enhancement strategies. 
It is important to note that each experiment is conducted three 
times for each model, and the results below represent the average 
outcomes to account for stochastic fluctuations in the model's responses.


\subsection{Results}
Both models show a strong performance in reconstructing the given 
UI layouts with an average of 88.96\% in code similarity
for GPT and 87.12\% for Gemini, as shown in Table~\ref{tab:table-performance}.
These results demonstrate that the models are capable of generating 
valid and visually similar HTML/CSS code from UI screenshots. 
While a more detailed evaluation of the visual fidelity is beyond 
the scope of this thesis, these results serve as a baseline 
for subsequent accessibility analysis.
Table~\ref{tab:table-performance} presents the number of accessibility 
violations in UI code generated by GPT and Gemini. Despite 
achieving high layout and structural fidelity, both models produce 
a significant amount of accessibility issues. On average, GPT 
generates 13.75 violations per UI, while Gemini produces 15.45, or 12.4\% 
more. The IR is 12.22\% for GPT and 11.42\% for Gemini,
while the IWIR is 47.10\% and 47.70\%, respectively. Although Gemini yields
more total violations than GPT on average, an analysis of its DOM size 
reveals that it generates larger code snippets, which explains
its lower IR because the violations are distributed over a larger 
number of nodes. However, the IWIR stays consistent across the two models, 
suggesting that the severity of the violations is similar.
These findings demonstrate a clear gap between visual correctness and 
accessibility compliance and highlight that without clear and 
explicit instructions, MLLMs might not be able to follow 
accessibility principles in code generation. To understand 
the underlying reasons for these accessibility issues, a manual 
analysis of the violations observed in the generated output
has been conducted. This process makes it possible to identify recurring 
failure patterns and model limitations.



\subsubsection{Most Common Violations}
The detailed analysis reveals that two primary categories of accessibility
issues are dominant in the generated code across both models.
The first category is insufficient color contrast. For instance,
color contrast violations are very common, causing 5.21 violations 
per UI for GPT and 6.89 for Gemini. The reason for this frequency 
can be attributed to the complexity of contrast checking, which 
requires the model to reason mathematically about RGB or hex values
and their luminance for the human eye, as defined by WCAG. This mathematical 
requirement is not only challenging for MLLMs, but also human 
developers often rely on automated tools to ensure compliance 
in this area. The main difficulty lies in the fact that 
even slight deviations in color selection can lead to significant 
usability barriers for visually impaired users. As shown in 
Figure~\ref{fig:common} (a), the generated code uses white text 
(\#ffffff) on a green background (\#67a52a and \#72b650)
or on a yellow background (\#cc9933). This results in 
contrast ratios of 3.00, 2.46, and 2.57, significantly below the
WCAG 2.1 minimum color contrast threshold of 4.5 for normal text.
Even though this mathematical task is within the capabilities of 
MLLMs, it remains a common challenge across both models because 
of their limited visual-perceptual understanding and mathematical 
precision.\newline
The second category is the lack or incorrect usage of semantic 
landmarks. To convert 
the visual structure of a webpage into a format that
assistive technologies can use, semantic landmarks, 
such as \texttt{<main>}, \texttt{<nav>},
\texttt{<header>}, and \texttt{<footer>}, are essential. 
However, these landmarks are not visually 
displayed in the UI and have no specific representation, making it 
difficult for MLLMs, which rely only on screenshot input,
to identify and generate them correctly. As 
shown in Figure~\ref{fig:common} (b), rather than using 
semantic landmarks, the model substitutes them with 
generic \texttt{<div>} elements. As a result, assistive technologies 
interpret the webpage as a flat structure, without navigational 
and structural details, which are necessary for keyboard users or 
screen reader navigation. This can significantly hinder 
accessibility, especially for users who rely on 
semantic segmentation to jump between regions of a webpage. 
% Essentially, the problem arises, first, due to the absence 
% of visible indicators, second, because of the model’s 
% limited exposure to the underlying meaning of UI 
% elements—the type of information human designers tend 
% to acquire through accessibility trainings or work experience.
The issue basically stems from two factors: first, the lack of 
visible indicators, and second, the model’s limited exposure to 
the underlying meaning of UI elements, which is the type of 
information that human designers often gain from accessibility training or work experience.

\begin{figure}[htbp]
  \centering
  % \includegraphics[width=0.8\linewidth]{images/common.png}
  \includegraphics[width=0.9\linewidth]{figures/colorcontrastlandmarkexample.png}
  \caption{Example of common accessibility violations.}
  \label{fig:common} 
\end{figure}


\begin{center}
\begin{tcolorbox}[colback=black!5!white,colframe=black!75!black,bottom=-0.05pt,top=-0.05pt]
\textit{\textbf{Finding:}} Even though both GPT and Gemini achieve high layout fidelity, 
they consistently generate code with accessibility violations, 
especially in color contrast and missing landmarks. This indicates 
a fundamental gap between visual accuracy and accessibility compliance.
\end{tcolorbox}
\end{center}


\begingroup
    \input{tables/violations.tex}
\endgroup



\section{RQ2: Do different MLLMs vary in their ability to generate accessible UI code?}
\subsection{Experiment Setup}
To understand how the choice of MLLMs influences the generation of 
accessible HTML/CSS, the analysis is extended beyond the two 
closed-sourced models used in RQ1. This included two additional 
open-source models, \textit{Qwen2.5-VL-7B} and \textit{LLaVA-7B}, 
selected for their strong performance in multimodal tasks and 
wide adoption in the research community. 
The 7B parameter variations are used to capture substantial 
differences in model behavior and performance while ensuring 
the feasibility under constrained computational resources and 
reproducibility by other researchers.
Similar to RQ1, all models are prompted with the same 
benchmark dataset and naive prompt,
which instructs the models to generate HTML/CSS from UI screenshots
without referencing accessibility. This setup allows a 
direct comparison across the models.

\subsection{Results}
Table~\ref{tab:table-performance} presents the average 
number of accessibility violations per UI, the IR, IWIR, and 
CodeSim for each model. Surprisingly, the open-source models
Qwen and Llava have the fewest violations, averaging 
6.21 and 5.79 violations per UI. Also, most of the other metrics 
show that these models perform better than the closed-source
models: the average values for the IR are 10.70\% for Qwen and
12.14\% for Llava, while the IWIR is 40.25\%
and 40.40\%, respectively.
Since these results 
are significantly lower than the results of GPT and Gemini, 
a closer analysis was conducted, and three key factors 
influencing the accessibility results were identified: 
training data biases, instructional alignments, and 
code generation capabilities.


\subsubsection{Training Data Biases.} Even if the 
internal training data of closed-source models is not publicly 
available, this qualitative analysis reveals noticeable differences 
in the models' behavior that can be attributed to training 
data biases.\newline
The first visible difference is the tendency of Gemini to produce 
more violations related to color contrast, averaging 
6.89 per UI for Gemini, compared to 5.21, 3.00, and 1.26 for GPT,
Qwen, and Llava. This discrepancy seems to be closely related to 
the diversity and origin of the colors used in the generated code.
For instance, Gemini shows a relatively limited color palette, 
consisting of only 41 unique colors across all UIs that cause 
violations. Over 40\% of these violations stem from the use 
of the \textit{primary blue} (\#007bff) color, which is 
commonly used in the Bootstrap framework~\cite{web:bootstrapv4}.
This strong dependency and bias towards a specific color suggests 
that Gemini's training data might have been disproportionately 
influenced by framework-centric codebases, such as those built 
with Bootstrap. As a result, the model shows a tendency to have 
stylistic preferences that do not align with accessibility.
For instance, the Bootstrap primary blue is often used for 
elements like links, buttons, or headings, without taking the 
background color into account. Especially combined with 
light-colored layouts, this increases the risk of insufficient 
color contrast. In contrast, GPT's color contrast violations are 
caused by a set of 83 distinct colors. The majority of these 
colors (71.47\%) are uniquely synthesized by the model and cannot 
be linked to any framework or library. Even if this 
independent color selection does not prevent color contrast 
violations, it tends to more closely match the visual 
design of the input UI. This can lead to improved contrast ratios,
given that the input UI is designed with accessibility in mind.
While this interpretation is speculative because of the 
black-box nature of the models, the observed patterns suggest 
a strong influence of the training data on the models' accessibility
outcomes.

\subsubsection{Instructional Alignments.} The second key 
factor appears to be the instructional alignment
and the behavior tuning of the models during training. GPT 
frequently generates code with accessibility-conscious elements
(e.g., alt-text for images or proper form labels), even when 
prompted without explicit instructions. As shown in 
Figure~\ref{fig:generalcapabilities} (a), GPT tends to produce 
descriptive alt-text and labels by default.\newline
While the reasons remain speculative, a plausible explanation 
could be GPT's accessibility-oriented use cases and 
alignment practices. For example, GPT has been used 
in real-world assistive technologies like \textit{Be My Eyes}~\cite{web:bemyeyes}. 
This application helps to interpret visual information into 
textual descriptions, which is crucial for visually impaired users.
This practical integration into a real-world scenario might 
have influenced the model's fine-tuning or evaluation objectives.
These improved accessibility considerations and sensitivity could 
be a result of accessibility-focused tasks during the training 
process of the model. This characteristic is less noticeable in 
other MLLMs.

\begin{figure}
  \centering
  \includegraphics[width=1\linewidth]{figures/generalcapability.png}
  \caption{Comparison of semantically rich (left) and structurally shallow (right) HTML snippets}
  \label{fig:generalcapabilities} 
\end{figure}

\newpage
\subsubsection{Code Generation Capabilities.} Despite generating
code with fewer violations, the open-source models Qwen and 
Llava do not necessarily produce accessible webpages.
The detailed analysis reveals that these models tend to generate 
simplistic and semantically shallow code snippets. They often 
generate flat layouts or generic tags, such as \texttt{<div>} or \texttt{<span>},
combined with little nesting, styling, or ARIA annotations as default. 
This behavior is illustrated in Figure~\ref{fig:generalcapabilities} (b),
where Llava reduces a multi-component UI layout into a linear 
sequence of \texttt{<div>} elements, following a common template.
This approach leads to fewer accessibility violations
because it lacks problematic elements and therefore reduces the 
surface for potential issues. However, this results in 
structurally incomplete and visually incorrect code, which can be 
seen in significant lower code similarity scores of 70.36\% for Qwen and
50.50\% for Llava, compared to 88.96\% for GPT and 87.12\% for Gemini.
This indicates that the smaller, open-source models fail to capture 
layout fidelity and semantic depth. This finding highlights the 
necessity to balance accessibility evaluation with structural 
quality, because the absence of content can artificially 
reduce the violation metrics.

\begin{center}
\begin{tcolorbox}[colback=black!5!white,colframe=black!75!black,bottom=-0.05pt,top=-0.05pt]
\textit{\textbf{Finding:}} Due to variations in alignment goals (e.g., GPT's accessibility-aware behavior) and training data composition (e.g., Gemini's dependence on framework-derived styles), accessibility violations differ significantly amongst MLLMs. Open-source models, such as Qwen and Llava, tend to produce simplified, under-specified code, which is the main reason why they report fewer violations.
\end{tcolorbox}
\end{center}


\section{RQ3: Does advanced prompt engineering lead to more accessible MLLM-generated
UI code?}
\subsection{Experiment Setup}
While the prior RQs explored the inherent capabilities of MLLMs with naive
prompting, this RQ investigated the prompting strategies' impact 
on the accessibility of the generated code. Prompt engineering has shown 
to be a powerful technique in prior work to improve and influence the 
behavior of LLMs, especially in structured tasks. Therefore, this study 
takes 7 different prompting strategies into account, ranging from 
naive to externally supported techniques. Note that the prompts follow 
the established best practices of prior research~\cite{suh2025accessiblecode, xiao2024interaction2code} and
that all prompting techniques are tested on GPT and Gemini by using the 
same benchmark dataset as in previous RQs.\newline

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\linewidth]{figures/prompts_new_new.png}
  \caption{Overview of the advanced prompting strategies.}
  \label{fig:prompts_new} 
\end{figure}


\noindent
\textbf{Naive Prompting (Baseline)}: 
  Without explicit accessibility instructions, the model is
  instructed to generate HTML/CSS code based on a screenshot of a webpage.\newline
\noindent
\textbf{Zero-Shot Prompting}: 
  The MLLMs are explicitly instructed to 
  generate accessible code by adding the instruction \textit{``comply with WCAG 2.1 standards''}, 
  as shown in Figure~\ref{fig:prompts_new}. 
  This explores the model's ability to generalize accessibility guidelines 
  only based on an instruction and without examples.\newline
\noindent
\textbf{Few-Shot Prompting}: 
  This strategy resembles the zero-shot prompt,
  but is enriched with several examples of WCAG 2.1 guidelines, combined 
  with a description of the task, an incorrect code snippet, and a 
  corresponding correct code snippet.
  The examples are sourced from the ARIA Authoring Practices Guide 
  (by W3C)~\cite{web:w3c_examples} and Accessible Components by GOV.UK
  Design System~\cite{web:govuk}, which are known for their accessibility
  best practices.
  Eight examples, which differ in their content (e.g., image alt-text, 
  form labels, landmarks), are manually collected. They serve as an 
  additional context for the model to lead the models towards a more 
  accessible code generation.\newline
\noindent
\textbf{Chain-of-Thought Prompting}: 
  As shown in Figure~\ref{fig:prompts_new}, 
  the prompt encourages the model to think step-by-step. 
  The objective is to guide the model to describe its 
  considerations in a structured plan and finally output the code.
  This helps to show intermediate reasoning steps that possibly 
  highlight accessibility-aware thinking.\newline
\noindent
\textbf{Agentic Prompting}: 
  As prior research~\cite{wu2023autogen} has shown, the use of multi-agent 
  systems, where each agent has a specific task, can lead to better
  results in complex tasks. Therefore, this strategy splits the 
  task of creating accessible code into three agents: The 
  \textit{Detector} agent is designed to detect accessibility violations 
  in a generated code, instructed with the naive prompt. It outputs a list
  of snippets with violations, including their location in the code. The 
  second agent, the \textit{Identifier}, is responsible for classifying 
  the violations into their respective WCAG guidelines (e.g., 
  color contrast, landmarks, etc.) and enriching each violation with 
  its severity level. The third agent, the \textit{Resolver}, is 
  instructed to resolve the list of violations in the code. In order to 
  prevent cascading errors, this agent is instructed to solve batches 
  of violations, having a size of \(n = 5\). The goal of this strategy 
  is to evaluate whether decomposing the task into smaller and more 
  manageable subtasks can improve the overall accessibility.\newline
\noindent
\textbf{React Prompting}: 
  This strategy first instructs the model to generate HTML/CSS code, 
  according to the naive prompt, and then to critique its accessibility
  violations detected by the automated tools. As shown in 
  Figure~\ref{fig:prompts_new}, the model is then asked to revise
  all violations found, prioritize those with the highest 
  severity, and output the revised code.
  This multi-step approach evaluates the capability of the model to 
  self-correct and iteratively apply accessibility standards. 
  Each self-refinement loop runs for three iterations or until 
  no further violations are detected.\newline
\noindent
\textbf{Color-Aware Prompting}: 
  As previous RQs have shown, color contrast violations are a common 
  issue in MLLM-generated code. Due to their mathematical nature, they
  can be difficult to solve. Therefore, this strategy extends the React 
  prompting by adding a color-aware step. It uses a pre-processing 
  step that extracts the color values from the screenshot, analyzes 
  the color contrast violations found, and provides the model with a 
  possible solution for each violation. Similar to the React prompting,
  the model is then instructed to output the revised code. 
  By supplying the model with extracted color values and recommended 
  replacements that satisfy WCAG thresholds, the objective is to 
  decrease color contrast violations. The model is 
  directed to generate code that maintains visual consistency while 
  utilizing compliant color pairs supplied by this analysis.
  Note that the refinement loop runs only for one iteration.\newline




\subsection{Results}
Table~\ref{tab:access-viol-models} presents the average number of accessibility 
violations (AV), total violations (TV), code similarity (CodeSim),
inaccessibility rate (IR), and impact weighted inaccessibility rate (IWIR).
The results demonstrate that advanced prompting techniques can significantly
reduce the number of accessibility issues in the generated code. For instance,
when using the most advanced prompt (React), 95.75\% of the violations for 
GPT and 82.07\% for Gemini are resolved, compared to their naive counterpart.
A similar trend can be observed for the IR and IWIR. While the 
IR is reduced to 0.44\% for GPT and 1.36\% for Gemini using the React 
prompt, the IWIR is reduced to 13.56\% and 21.48\%, respectively. However, 
the effectiveness of each prompting strategy varies across the models, and 
some strategies are more suited for certain models than others.


\begingroup
    \input{tables/violations_allprompts.tex}
\endgroup

\subsubsection{Impact of Zero-Shot Prompting}
All models show a moderate, but measurable improvement in accessibility 
violations when a simple instruction about accessibility is added to 
the prompt. For instance, GPT reduces the average amount of 
violations by 10.33\% and Gemini by 7.76\% compared to the 
naive prompt. However, while the IR shows a similar trend with 
a considerable reduction for both models, the IWIR remains almost 
unchanged, even showing a slight increase. This suggests that even 
if the models are capable of activating their accessibility 
awareness, even without explicit examples, they still struggle to 
solve the most severe violations.\newline
A deeper analysis of the violation types and their distribution 
reveals that some limitations for the models remain. For example,
both models often use accessibility-related attributes (e.g., 
label, alt, role, aria-*) but misuse them and fail to apply them 
in a consistent manner. Figure~\ref{fig:zero-shot-example} demonstrates
a representative example. While Gemini generates an e-mail 
subscription form field, it fails to provide any accessible name,
e.g., neither a \texttt{<label>} nor an \texttt{aria-label} attribute.
This indicates that zero-shot prompting triggers awareness, it 
does not necessarily resolve accessibility gaps and inconsistencies.

\enlargethispage{2\baselineskip}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{figures/zeroshot-example.png}
  \caption{Accessibility gap in zero-shot prompting: omission of an accessible label in the generated code compared to the correctly labeled ground truth.}
  \label{fig:zero-shot-example} 
\end{figure}

\subsubsection{Impact of Few-Shot Prompting}
The few-shot prompting technique results in a 30.98\% improvement
in accessibility performance for GPT, with the average number 
of violations dropping from 13.75 to 9.49, while the IR 
decreases from 12.22\% to 7.96\%. By observing eight 
high-quality, concrete examples, the model is capable of 
copying the underlying patterns and structure. For instance,
when provided with an example of correct heading structures, such 
as \texttt{<h1>}, GPT is able to incorporate this pattern into its
generation, reducing the average number of \emph{Headings} violations 
from 26 to 15. Furthermore, the examples presented to the models
reflect only important accessibility guidelines, which 
significantly impact users and therefore also have a high 
severity. This is reflected in the IWIR, which drops from 
47.10\% to 44.64\%.
This demonstrates the importance of concrete 
examples in guiding the model, especially for concepts like
accessibility, which may be underrepresented in the training data.\newline
However, surprisingly, Gemini does not show similar improvements.
It performs slightly worse with an increase of 1.88\% in the average
number of violations. Based on the qualitative analysis, this
effect can be attributed to two primary factors. First, Gemini
exhibits a 56.52\% increase in violations related to the
\emph{Distinguishable Links} category. Due to its exposure
to link-related few-shot examples, it tends to generate links more 
frequently, but often without sufficient contrast or 
styling, violating the WCAG requirements. This phenomenon 
may be directly linked to the color bias in the training data,
which has been observed in RQ2. Gemini tends to use generic, default 
colors for links, such as the primary blue from the Bootstrap framework,
which used to be the default color for links in this framework. 
Since it also fails to apply distinct stylings for the links, this
combination leads to 
a lack of distinction between links and the surrounding text.
Figure~\ref{fig:distinguishablecolorlinks} illustrates an example, where 
Gemini generates links with insufficient contrast and no distinct styling.
\newline
The second violation category 
\emph{Landmark \& Region} increases by 12.83\%, due to 
Gemini's tendency to misuse the semantic 
elements, such as \texttt{<main>} or \texttt{<nav>}. This 
can likely be attributed to a misinterpretation of 
the few-shot examples, which might be replicated 
directly without a deeper semantic understanding of 
how those elements contribute to accessibility.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{figures/distinguishablecolorlinks.png}
  \caption{Example of a Distinguishable Links violation in Gemini’s few-shot output. The generated code does not include the underline and relies solely on a low-contrast orange (1.97:1), whereas the ground truth includes the underline to ensure link distinction despite insufficient contrast.}
  \label{fig:distinguishablecolorlinks} 
\end{figure}

\subsubsection{Impact of Chain-of-Thought Prompting}
Chain-of-Thought (CoT) prompting results in notable improvements 
over the naive prompt, i.e., averaging 10.04 vs 13.75 violations
per UI for GPT and 14.28 vs 15.45 for Gemini. Due to 
the reasoning instruction before generating code, 
the models are able to reflect possible accessibility 
constraints often missed in direct code generation.
Interestingly, GPT benefits significantly more 
than Gemini, with a relative improvement of 26.98\% vs 
7.57\%. This is probably because GPT constantly 
outlines an organized structure to reasoning by 
recognizing particular accessibility issues 
(such as contrast ratios and alt text) and integrating them into 
the code that is generated. In contrast, Gemini's 
CoT reasoning is less consistent and frequently omits 
key steps or fails to use the reasoning effectively 
during code generation.

\subsubsection{Impact of Agentic Prompting}
Splitting the task into three agents only leads to 
moderate improvements, as GPT reduces its violations 
by 17.53\% and Gemini by 14.82\% compared to the naive 
baseline. Similar to the prior strategies, the IR follows 
the same trend, dropping from 12.22\% to 8.86\% for GPT (27.50\%) and
from 11.42\% to 8.36\% for Gemini (26.80\%), but the IWIR decreases much less,
with 11.25\% for GPT and only 1.47\% for Gemini.\newline
The qualitative analysis reveals two main reasons for these 
results. First, the Detector agent misses some 
violations in the generated code and occasionally even reports 
false positives. As a result, the Resolver agent 
either does not resolve all violations within the code 
or changes already correct markup, which leads to new 
violations. Second, the Resolver resolves the violations in 
batch sizes. This can prevent the Resolver from understanding 
layout dependencies, leading to small but significant changes 
that decrease the layout fidelity, which results in the 
lowest code similarity of all prompting techniques
(86.12\% for GPT and 85.07\% for Gemini). In general, 
while the agentic prompting reduces the load per step of 
the models, it introduces additional noise that 
propagates through the agents.

\subsubsection{Impact of React Prompting}
The experiment shows that both GPT and Gemini significantly 
improve their accessibility performance when using the React 
prompting strategy. Across three iterative refinement 
cycles, GPT achieves reductions of 82.33\%, 92.07\%, and 95.05\% 
in accessibility violations compared to the naive prompt, while 
Gemini improved by 62.14\%, 77.67\%, and 82.07\%, respectively.
Similar improvements can be observed for the IR and IWIR. 
After three iterations, only 0.44\% of nodes in GPT's code
contain accessibility violations, while Gemini remains with 1.36\%.
Interestingly, not only the number of violations but also 
their severities are reduced. For instance, the IWIR drops from 
47.10\% to 13.56\% for GPT and from 47.70\% to 21.48\% for Gemini.
This indicates that the model is not only capable of 
resolving violations, but also prioritizes the violations 
with severity \emph{serious} or \emph{critical}. After three 
refinement iterations, neither GPT nor Gemini produces 
any violations with severity \emph{critical}, and only 
few with \emph{serious}.\newline 
The results suggest that even if models do not inherently 
obey all accessibility guidelines, they can be
guided towards more accessible code by incorporating 
external observations of the automated tools. GPT 
consistently outperforms Gemini across all iterations,
possibly due to its stronger alignment with 
Reinforcement Learning from Human Feedback (RLHF), which 
equips it with a better capacity to self-correct and 
obey instructions based on user-oriented feedback.\newline 
A notable observation from the React prompting is that the 
two main categories of accessibility violations, 
color contrast and landmarks, are significantly reduced.
While landmark violations are reduced by 98.21\% for GPT and
95.85\% for Gemini, color contrast violations are reduced by
90.58\% and 64.93\%, respectively. 
These results reflect a growing capacity of MLLMs 
to add feedback into their stylistic choices, which 
leads to more accessible and visually consistent webpages.\newline 
However, the React prompting also has its limitations.
Even though both 
models decrease the dependency on framework-based 
colors, the results still 
vary across the models. For instance, especially 
during the first refinement iterations, Gemini still 
heavily relies on framework-based color palettes, 
especially Bootstrap's primary blue, which leads to 
an increase in those violations of 23\% with 
this exact color, even though overall the amount 
of color contrast violations is reduced by 22\%. 
This finding suggests that the model might not be able to 
synthesize its own new colors, but rather tries to
substitute problematic colors with known ones
from its training data.
Furthermore, some
attempts to fix particular violations can 
occasionally lead to new problems elsewhere in 
the code. As Figure ~\ref{fig:iterativecascadingissues} suggests, the model 
successfully solves certain violations (for example, adds 
a \texttt{<label>} to the form element), but at the same time 
introduces a new issue with the color contrast of the label.
These cascading errors may arise from a limited 
understanding of the code's underlying structure and 
dependencies, which highlights a potential limitation 
of current MLLMs.

\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/iterativecascadingissues.png}
  \caption{Login screen before (left) and after (right) the first refinement iteration. The model successfully solves the missing label violation, but introduces a new issue with the color contrast of the label.}
  \label{fig:iterativecascadingissues} 
\end{figure}

\subsubsection{Impact of Color-Aware Prompting}
Enhancing the React prompting technique with pre-computed 
and WCAG-compliant colors has a different impact on both models.
While Gemini struggles to invent new colors, it 
benefits the most with its average number of violations 
dropping by 83.30\%, compared to the naive prompt, and 
is even able to reduce its number by 6.86\%, compared to 
the React prompting. While GPT also benefits from this 
strategy, by reducing its violations by 75.64\%, it does not
match the improvements of the React prompting strategy.\newline
Two main reasons can be attributed to this finding. First,
GPT is already able to synthesize new colors. Providing
new externally computed colors occasionally leads to
conflicts elsewhere in the code, as the model tries to
incorporate these colors. Second, the bounding-box detection
of \textit{Design2Code} occasionally groups elements
together that are not visually connected in the UI. This
can lead to situations where the model applies external
colors for an entire region, rather than a specific node.
Overall, the results suggest that adding explicit WCAG-compliant
colors can significantly lower contrast violations. However,
the underlying model's design capabilities and accuracy
determine how beneficial this strategy is.


\begin{center}
\begin{tcolorbox}[colback=black!5!white,colframe=black!75!black,bottom=-0.05pt,top=-0.05pt]
\textit{\textbf{Finding:}} Prompting strategies significantly 
influence the accessibility of MLLM-generated UI code. 
While the models benefit from explicit guidance, 
advanced techniques like React prompting yield the most 
substantial improvements. However, each strategy may also 
introduce model-specific limitations and, in some cases, 
create new issues. These findings underscore the importance 
of aligning prompting strategies with model capabilities 
to enhance accessibility outcomes.
\end{tcolorbox}
\end{center}


\section{RQ4: How consistent are the accessibility violations across different MLLMs on the same UI screenshot?}
\subsection{Experiment Setup}
The objective of this RQ is to find out whether different MLLMs 
cause similar or complementary accessibility violations when 
asked to generate HTML/CSS from the same UI screenshot.
Consistent error patterns would indicate that systematic 
limitations, caused by similar training data, exist. 
In contrast, complementary violations would motivate 
ensemble or shared approaches between the models to improve the 
overall accessibility of the generated code.
To investigate this, combined with 
the naive prompting technique, the benchmark dataset is used as 
input for GPT and Gemini. For every tuple of
screenshot and model, the list of accessibility violations
is extracted, sorted and, represented by a violation 
vector, which counts the number of issues per 
WCAG class. 



\newcommand{\vect}[1]{\begin{pmatrix}#1\end{pmatrix}}
\newcommand{\issues}{k}                   % Anzahl der Issue-Klassen
\newcommand{\vx}{\mathbf x}
\newcommand{\vy}{\mathbf y}

\textbf{Violation Vectors:}  
  Each violation vector is a numerical representation of the accessibility 
  violations found in the generated HTML/CSS. Given a model-screenshot pair, each 
  dimension corresponds to a specific violation class, and its value 
  represents how many violations of that class were found. This representation 
  makes comparing error patterns across different models and
  screenshots easier. The violation vector is defined as follows:
  \[
  \left(
    \begin{array}{@{}l r@{}}
      \text{Issue}_{1}: & x_{1}\\
      \text{Issue}_{2}: & x_{2}\\
      \vdots           & \vdots\\
      \text{Issue}_{\issues}: & x_{\issues}
    \end{array}
  \right)
  \xmapsto{\text{to vector}}
  \mathbf v \;:=\;
  \vect{x_{1}\\x_{2}\\\vdots\\x_{\issues}} \in \mathbb R^{\issues}.
\]

To compare the similarity of the violation vectors, the 
cosine similarity is used, a common metric in 
other research fields. 

\textbf{Cosine Similarity}  
  Given two vectors with $\vx,\vy\in\mathbb R^{\issues}$, the cosine similarity is defined as:
  \[
  \operatorname{cos\_sim}(\mathbf v_1,\mathbf v_2)=
  \frac{\mathbf v_1^{\mathsf T}\mathbf v_2}{\lVert \mathbf v_1\rVert\,\lVert \mathbf v_2\rVert}
  \quad\in[0,1].
\]


\subsection{Results}
Figure~\ref{fig:cosinesimilarity} illustrates the cosine similarity matrix 
plotted as a heatmap, where each cell represents the similarity between 
the violation vectors of different model runs. 
Overall, most cells 
are light-colored, indicating a high cosine similarity 
between the runs of the models, suggesting that GPT and Gemini
produce similar violation patterns.\newline
This is especially true 
for the upper part of the heatmap, representing the 
dataset entries of the Design2Code dataset (examples 1-28). 
Here, both models 
show a high degree of consistency and similarity across 
each other and also across the individual runs.
The lower part of the heatmap, representing the
dataset entries from the dataset WebCode2m, displays 
a more diverse pattern, with a mix of lighter and 
darker cells. Nonetheless, even in this section, 
the majority of the cells are light-colored, showing
that the models are still largely consistent in their violation 
patterns.\newline
A manual inspection of the darker cells reveals 
that a great number of lower similarity scores stems from 
two main reasons: 1) In some runs, a model is capable 
of generating code with only few violations, leading to 
a violation vector with many zeros or small values.
Because the cosine similarity is highly sensitive to 
small magnitudes, even another run with a comparable low
number of violations can result in a lower cosine similarity score. 
This effect is especially true for WebCode2m examples, where the models generally 
produce fewer violations. 
2) The second reason for lower similarity scores stems 
from possible recurring errors in the code generation. 
For instance, if one run uses a non-WCAG-compliant color 
palette, the resulting number of color contrast violations 
might be considerably higher than in other runs with 
more compliant colors.
This difference in a single category can significantly 
reduce the cosine similarity, even if other categories
show comparable patterns.\newline 
Overall, the results suggest that the models generate 
code snippets with very similar numbers, 
categories, and distributions of accessibility violations. 
The high similarity score indicates that the models share systemic
limitations and biases, which are likely to be inherited 
from the underlying training data.

\begin{center}
\begin{tcolorbox}[colback=black!5!white,colframe=black!75!black,bottom=-0.05pt,top=-0.05pt]
\textit{\textbf{Finding:}} Accessibility violations 
are highly consistent across GPT and Gemini, with an 
average cosine similarity of 0.719. This suggests that 
the models inherit the same systemic blind spots from 
web-scale training data.
\end{tcolorbox}
\end{center}


\begin{figure}
  \centering
  \includegraphics[width=0.4\linewidth]{figures/cosinesim.png}
  \caption{Cosine similarity heatmap between the 3 runs of GPT and Gemini on the same UI screenshot. 
  Each cell represents the cosine similarity of the accessibility violation vectors generated by the models.
  Light = high similarity, dark = low similarity.}
  \label{fig:cosinesimilarity} 
\end{figure}





\section{RQ5: Does data leakage affect the accessibility of MLLM-generated UI code?}
\subsection{Experiment Setup}
A concern that is often raised in the context of MLLMs is 
the possibility of data leakage, where the models have 
already been exposed to the test data during pretraining, 
thus leading to inflated performance. This RQ aims to 
investigate whether such leakage has an impact on the 
accessibility of the generated code.\newline
To investigate this, two distinct datasets have been created 
to compare model performance. 1) \textbf{Benchmark Dataset},
which has been used and described in the previous RQs. 2)
\textbf{Synthetic Dataset}, which consists of two distinct 
subsets. The first subset is
created using layout mutation and
hybrid synthesis techniques. Therefore,
heuristic transformation rules have been created to
mutate layout structures, elements arrangements,
component colors, and text content. In a second step,
some layouts and components are then interchanged across 
the UIs, which leads to new compositions. 
It contains new
UI compositions and code that are not likely to have
exact matches in the pretraining data.
An example 
of this synthetic dataset is shown in Figure~\ref{fig:dataleakagemutation}.
As a result, 10 webpages are synthesized, which are 
rendered into screenshots and paired with their 
corresponding mutated HTML/CSS code. 
The second subset is created to complement 
more up-to-date and real-world examples in the synthetic 
dataset. This subset is curated by selecting two 
open-source web projects~\cite{web:ecommerceferhan, web:alphaonelabsedu}
that are created after the official knowledge cutoff 
dates of the MLLMs. These projects contain new samples 
that are unlikely to have been seen during pretraining.
However, web code from open repositories often 
includes dynamically generated content, external dependencies, 
and redundant content, which can bias this accessibility-focused 
analysis negatively. Therefore, the collected code 
is manually cleaned according to the previous 
steps, described in Section~\ref{subchapter:Dataset}.
Through this process, 10 new webpages are collected, and
their corresponding HTML/CSS together with the rendered UI screenshots 
are added to the subset.\newline
The two subsets are then combined into a single 
synthetic dataset, which is used to compare the 
models' performance against the benchmark dataset.\newline
All experiments use the same naive prompting setup 
and are conducted on GPT and Gemini.
By comparing the code similarity and distribution of 
accessibility violations across the 
different datasets, it can be evaluated 
whether the performance variations suggest
memorization or data leakage effects.

\begin{figure}
  \centering
  \includegraphics[width=0.9\linewidth]{figures/dataleakage.png}
  \caption{Original web page (left), a heuristically mutated version (middle) with altered color, layout and text, and a cross-site hybrid composition combining sections from multiple pages (right).}
  \label{fig:dataleakagemutation} 
\end{figure}

\begingroup
    \input{tables/dataleakage.tex}
\endgroup

\subsection{Results}
Table~\ref{tab:dataleakage} shows the results across all 
three datasets. Overall, the code similarity scores are 
relatively consistent across the datasets, even showing slightly 
higher values for the synthetic dataset. While GPT 
achieves a code similarity of 88.96\% for the benchmark dataset vs 
89.17\% for the synthetic dataset, Gemini
achieves 87.12\% vs 88.01\%, respectively. This 
suggests that the models are not merely memorizing and 
reproducing the training data, but rather are 
able to generalize and synthesize new code. The 
IR changes by 0.5-0.7 percentage points (GPT 
slightly lower, Gemini slightly higher), which is 
considered within the variance of the models 
shown in RQ3. This indicates that the number of 
DOM nodes with at least one accessibility violation 
is largely unaffected by the underlying dataset.
Lastly, both models show a modest increase in the 
IWIR, with GPT increasing from 47.10\% to 49.47\% 
and Gemini from 47.70\% to 50.30\%. Manual inspection 
reveals that the synthetic dataset, especially 
the 10 mutated UIs, contains more composite sections, 
which tend to trigger slightly more color 
contrast violations, and therefore lead 
to a higher IWIR.\newline
These findings demonstrate comparable performances 
in both code fidelity and accessibility metrics across
the different datasets. This suggests that the results 
shown in previous RQs are not artificially inflated 
by data leakage, but rather reflect the overall 
generalization capabilities of the models. Therefore,
it can be concluded that data leakage is unlikely to 
exist or affect the evaluation process of MLLMs in 
this thesis.

\begin{center}
\begin{tcolorbox}[colback=black!5!white,colframe=black!75!black,bottom=-0.05pt,top=-0.05pt]
\textit{\textbf{Finding:}} The consistent performance of 
MLLMs across the benchmark and synthetic datasets 
suggests that data leakage is minimal 
and the models' behavior observed in earlier 
experiments is not a result of memorization 
or pretraining exposure.
\end{tcolorbox}
\end{center}
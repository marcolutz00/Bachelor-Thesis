\chapter{Methodology}\label{chapter:Methodology}

In this section, the general methodology for evaluating the 
accessibility of HTML/CSS code is described. It includes a combination
of automated auditing tools and manual verification. This hybrid
approach ensures to identify a wide range of accessibility issues,
reconcile inconsistencies across tools and guarantee 
reliable results. All evaluations are conducted in accordance
with WCAG 2.1, which is the most widely adopted standard 
supported by the tools at the time of this thesis.

\subsection{Automated Accessibility Evaluation}
A wide range of automated accessibility checkers are available to 
detect violations in web content.
According to former studies, automated testing can detect up to $\sim$60\% 
of accessibility issues~\cite{deque2023accessibility}. This makes them 
valuable for developers, however they can not replace manual testing or 
expert review completely.
However in practice, a combination of various tools can help to minimize the 
oversight of accessibility violations during the tests.\newline
This study uses three widely adopted automated accessibility evaluation 
tools: \textit{Axe-Core}, \textit{Google Lighthouse Accessibility} 
and \textit{Pa11y}. Each tool offers unique detection mechanisms,
coverage areas and reporting formats.
\begin{itemize}
  \item \textbf{Axe-Core (4.10.3)~\cite{web:axecore}:} is a rule-based engin
  developed by Deque Systems and commonly integrated into browser 
  extensions and CI/CD pipelines. It provides detailed issue classifications 
  and links each violation to an own rule-set.
  \item \textbf{Google Lighthouse Accessibility (12.4.0)~\cite{web:lighthouse}:} 
  embedded within Chromium-based browsers, performs accessibility audits
  alongside performance and SEO diagnostics.
  \item \textbf{Pa11y (8.0.0)~\cite{web:pa11y}:} is a flexible command-line tool 
  that uses HTML CodeSniffer as its engine and is especially useful for batch 
  evaluation of static pages. During some tests on the benchmark dataset, we 
  found out that Pa11y has especially strengths in areas where the other tools
  seem to fail. Therefore, we decide to use it as our third complementary tool.
\end{itemize}
 
All three tools were used to evaluate the generated HTML/CSS code. 
However, the outputs of each tool may differ. For example, 
Pa11y flagged a missing label for a form field under WCAG
2.1 Technique F68 (“This form field should be labelled in some way.”), 
while Axe-core
reported the same issue using its own rule IDs (e.g. “label”) and includes 
WCAG success-criterion
tags such as “4.1.2 Name, Role, Value” in the rule metadata, but it does not cite WCAG
techniques. This discrepancy reflects a common challenge in automated accessibility evaluation,
that the tools often differ in their interpretation, granularity and labeling of the same underlying
issues.\newline
To address this inconsistency and improve cross-tool comparability, a 
unified taxonomy of accessibility issues is created. In order to 
aggregate the outputs of the three tools, first all 
detected violations are compiled. Based on the preserved 
metadata, such as rule IDs, WCAG mappings, descriptions and affected
HTML elements the violations are grouped into functionally 
equivalent categories (e.g. missing labels, insufficient color contrast,
missing alt-text). Next, overlapping rule definitions are merged 
into a single rule, leading to a consistent set of 40 
accessibility categories, each mapped to one or more WCAG 
success criteria. For instance, issues such as “H43.*” 
(from pa11y) and “empty-table-header” (from axe-core) were all
grouped under the category "Table Headers". Similarly, 
contrast-related violations were unified
under WCAG 2.1 1.4.3 Contrast (Minimum), 
regardless of variation in wording across tools.

\subsection{Manual Verification}
As mentioned, automated tools can not replace manual testing completely. 
Certain accessibility guidelines, especially those involving 
contextual understanding or semantic meaning still require 
human judgment. Therefore, we support the evaluation with a 
structured manual review process. During the experiments, the 
generated HTML/CSS is audited manually by using the previously 
created accessibility issue taxonomy, especially identifying 
violations that may be overlooked by automated tools 
(e.g. improper headings, redundant links and semantically 
ambiguous structures). This manual layer of analysis helps 
to provide a more comprehensive and realistic assessment of
the accessibility quality.